{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevoniano/AI-stuff/blob/master/DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk_Z0mqd1_tL",
        "colab_type": "text"
      },
      "source": [
        "# DQN con Keras y AI Gym\n",
        "\n",
        "En este notebook vamos a usar AI Gym y Keras para implementar el algoritmo DQN. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Keras es una librería de alto nivel para la construcción de redes neuronales que usa como back end Tensorflow. \n",
        "\n",
        "AI Gym es una librería que \"empaqueta\" escenarios complejos y expone unos cuántos métodos para interactuar con ellos. \n",
        "\n",
        "En este ejemplo usaremos [cartpole-v1](https://www.youtube.com/watch?v=0fNjV2icRd0). La tarea es usar dos acciones (izquierda y derecha) para mantener vertical una plataforma sobre la que hay un péndulo invertido. \n",
        "\n",
        "La [documentación en Github](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py) muestra que el estado se conforma por 4 variables, angulo y su derivada, posicion en x y su derivada. \n",
        "\n",
        "El algoritmo recibirá un estado y deberá generar una salida: 0 o 1, indicando al programa hacia donde moverse. \n",
        "\n",
        "Por cada paso que el péndulo se mantenga cerca de la vertical **el agente recibe 1 punto**, por lo que para maximizar su recompensa debe mantenerse vertical el mayor número posible de pasos. **El máximo es 501.**\n",
        "\n",
        "El experimento tambien se puede correr offline [clonando este repositorio](https://github.com/jonaths/dqn-diplomado). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGC5dSUE5FFw",
        "colab_type": "text"
      },
      "source": [
        "## Importar dependencias\n",
        "Keras, AI Gym y Numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbLij5B560dD",
        "colab_type": "code",
        "outputId": "4b7cd913-54a1-4985-d8f3-8dd326e0f90b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import gym\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from collections      import deque\n",
        "from keras.models     import Sequential\n",
        "from keras.layers     import Dense\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8gWLsla4GYg",
        "colab_type": "text"
      },
      "source": [
        "## Instanciar al agente\n",
        "Esta clase construye la red neuronal y contiene los métodos con los que el agente aprende y guarda el modelo entrenado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4eSmVwX63hY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent():\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.weight_backup      = \"cartpole_weight.h5\"\n",
        "        self.state_size         = state_size\n",
        "        self.action_size        = action_size\n",
        "        # tamano del replay memory\n",
        "        self.memory             = deque(maxlen=2000)\n",
        "        self.learning_rate      = 0.001\n",
        "        self.gamma              = 0.95\n",
        "        self.exploration_rate   = 1.0\n",
        "        self.exploration_min    = 0.01\n",
        "        self.exploration_decay  = 0.995\n",
        "        # instanciar la red neuronal al crear el modelo\n",
        "        self.brain              = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        \"\"\"\n",
        "        Construye el modelo o lo carga desde un archivo h5\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
        "\n",
        "        if os.path.isfile(self.weight_backup):\n",
        "            model.load_weights(self.weight_backup)\n",
        "            self.exploration_rate = self.exploration_min\n",
        "        return model\n",
        "\n",
        "    def save_model(self):\n",
        "        # guardar el modelo\n",
        "        self.brain.save(self.weight_backup)\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        Recibe un estado y regresa una accion \n",
        "        :param state: un vector con shape (1, 4)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # epsilon greedy\n",
        "        if np.random.rand() <= self.exploration_rate:\n",
        "            return random.randrange(self.action_size)\n",
        "        # un vector con los valores Q de las dos posibles acciones\n",
        "        # [[20.36317  20.821508]]\n",
        "        act_values = self.brain.predict(state)\n",
        "        # regresa el indice de la accion con mayor valor\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Guardar una transaccion en replay memory\n",
        "        :param state: las 4 variables que representan el estado en cartpole\n",
        "            [[-0.02476985 -0.04408725 -0.0043659   0.03441053]]\n",
        "        :param action: el indice de la accion\n",
        "            0 o 1\n",
        "        :param reward: la recompensa\n",
        "            1.0\n",
        "        :param next_state: el nuevo estado\n",
        "            [[-0.02565159  0.15109704 -0.00367769 -0.25964668]]\n",
        "        :param done: si el episodio termino\n",
        "            True o False\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        " \n",
        "    def replay(self, sample_batch_size):\n",
        "        \"\"\"\n",
        "        Entrena sacando informacion de replay memory\n",
        "        :param sample_batch_size: el tamaño del mini batch\n",
        "        :return: \n",
        "        \"\"\"\n",
        "        # si no hay suficientes muestras no hace nada\n",
        "        if len(self.memory) < sample_batch_size:\n",
        "            return\n",
        "        # toma una muestra aleatoria de replay memory\n",
        "        sample_batch = random.sample(self.memory, sample_batch_size)\n",
        "        \n",
        "        # entrena la red neuronal con el mini batch\n",
        "        for state, action, reward, next_state, done in sample_batch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = reward + self.gamma * np.amax(self.brain.predict(next_state)[0])\n",
        "            target_f = self.brain.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            \n",
        "            # entrenar: este estado (st ate) debe tener este valor (target_f)\n",
        "            # fit resta la estimacion actual, asi solo pasamos el target\n",
        "            self.brain.fit(state, target_f, epochs=1, verbose=0)\n",
        "            \n",
        "        # actualiza epsilon\n",
        "        if self.exploration_rate > self.exploration_min:\n",
        "            self.exploration_rate *= self.exploration_decay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMdzbC5Q4WWs",
        "colab_type": "text"
      },
      "source": [
        "## Construir el experimento\n",
        "Esta clase instancía el escenario de AI Gym y corre el experimento. \n",
        "\n",
        "Entradas:\n",
        "- episodes: el numero de episodios (recomendado 10,000)\n",
        "- batch_size: el tamaño del minibatch (recomendado 32)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqxO5JsN6-Nt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CartPole:\n",
        "    def __init__(self):\n",
        "        self.sample_batch_size = 32\n",
        "        self.episodes          = 10000\n",
        "        self.env               = gym.make('CartPole-v1')\n",
        "\n",
        "        self.state_size        = self.env.observation_space.shape[0]\n",
        "        self.action_size       = self.env.action_space.n\n",
        "        self.agent             = Agent(self.state_size, self.action_size)\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        print(\"=== Running \")\n",
        "        try:\n",
        "            sample_results = []\n",
        "            for index_episode in range(self.episodes):\n",
        "                state = self.env.reset()\n",
        "                state = np.reshape(state, [1, self.state_size])\n",
        "\n",
        "                done = False\n",
        "                index = 0\n",
        "                while not done:\n",
        "                    # self.env.render()\n",
        "\n",
        "                    action = self.agent.act(state)\n",
        "\n",
        "                    next_state, reward, done, _ = self.env.step(action)\n",
        "                    next_state = np.reshape(next_state, [1, self.state_size])\n",
        "                    self.agent.remember(state, action, reward, next_state, done)\n",
        "                    state = next_state\n",
        "                    index += 1\n",
        "                    \n",
        "                # descomentar para imprimir cada episodio\n",
        "                # print(\"Episode {}# Score: {}\".format(index_episode, index + 1))\n",
        "                \n",
        "                # para la estadistica\n",
        "                sample_results.append(index + 1)\n",
        "                \n",
        "                # imprime cada 25 episodios\n",
        "                if index_episode > 0 and index_episode % 25 == 0:\n",
        "                    print(\"Episode {}# Avg Score: {}, Min: {}, Max: {} \"\n",
        "                          .format(index_episode, \n",
        "                                  sum(sample_results) / len(sample_results), \n",
        "                                  min(sample_results), max(sample_results))\n",
        "                         )\n",
        "                    sample_results = []\n",
        "                self.agent.replay(self.sample_batch_size)\n",
        "        finally:\n",
        "            self.agent.save_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQumy4mB4je4",
        "colab_type": "text"
      },
      "source": [
        "## Correr el experimento\n",
        "Estos dos comandos instancian el escenario y corren el experimento. \n",
        "\n",
        "Despues de los 4000-5000 episodios  el programa devolverá resultados con maximos de 501 puntos. Esto quiere decir que hubo episodios donde el péndulo se mantuvo vertical por 501 episodios. \n",
        "\n",
        "Se puede subir un archivo de pesos cartpole_weight.h5 y el programa lo usará."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaOkmaxW7Er9",
        "colab_type": "code",
        "outputId": "ae0750cd-6e31-407e-be0d-095a035a77bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "cartpole = CartPole()\n",
        "cartpole.run()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0629 17:44:08.565289 140465407723392 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0629 17:44:08.600854 140465407723392 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0629 17:44:08.611209 140465407723392 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0629 17:44:08.667808 140465407723392 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0629 17:44:08.688434 140465407723392 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "W0629 17:44:08.690799 140465407723392 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "=== Running \n",
            "Episode 25# Avg Score: 25.346153846153847, Min: 10, Max: 54 \n",
            "Episode 50# Avg Score: 17.8, Min: 9, Max: 31 \n",
            "Episode 75# Avg Score: 17.0, Min: 9, Max: 37 \n",
            "Episode 100# Avg Score: 18.2, Min: 11, Max: 42 \n",
            "Episode 125# Avg Score: 15.04, Min: 10, Max: 32 \n",
            "Episode 150# Avg Score: 22.96, Min: 10, Max: 65 \n",
            "Episode 175# Avg Score: 34.08, Min: 14, Max: 65 \n",
            "Episode 200# Avg Score: 16.4, Min: 10, Max: 36 \n",
            "Episode 225# Avg Score: 31.4, Min: 10, Max: 78 \n",
            "Episode 250# Avg Score: 29.32, Min: 10, Max: 155 \n",
            "Episode 275# Avg Score: 29.96, Min: 12, Max: 99 \n",
            "Episode 300# Avg Score: 25.52, Min: 9, Max: 77 \n",
            "Episode 325# Avg Score: 13.96, Min: 10, Max: 48 \n",
            "Episode 350# Avg Score: 28.84, Min: 12, Max: 54 \n",
            "Episode 375# Avg Score: 25.4, Min: 9, Max: 89 \n",
            "Episode 400# Avg Score: 37.12, Min: 9, Max: 100 \n",
            "Episode 425# Avg Score: 30.6, Min: 16, Max: 47 \n",
            "Episode 450# Avg Score: 76.84, Min: 23, Max: 213 \n",
            "Episode 475# Avg Score: 95.8, Min: 26, Max: 205 \n",
            "Episode 500# Avg Score: 129.96, Min: 49, Max: 369 \n",
            "Episode 525# Avg Score: 115.84, Min: 19, Max: 340 \n",
            "Episode 550# Avg Score: 171.28, Min: 40, Max: 424 \n",
            "Episode 575# Avg Score: 94.04, Min: 16, Max: 415 \n",
            "Episode 600# Avg Score: 108.44, Min: 23, Max: 452 \n",
            "Episode 625# Avg Score: 163.96, Min: 28, Max: 501 \n",
            "Episode 650# Avg Score: 128.68, Min: 17, Max: 501 \n",
            "Episode 675# Avg Score: 108.8, Min: 16, Max: 501 \n",
            "Episode 700# Avg Score: 54.2, Min: 12, Max: 114 \n",
            "Episode 725# Avg Score: 117.64, Min: 21, Max: 204 \n",
            "Episode 750# Avg Score: 179.44, Min: 76, Max: 501 \n",
            "Episode 775# Avg Score: 140.08, Min: 24, Max: 501 \n",
            "Episode 800# Avg Score: 79.84, Min: 24, Max: 126 \n",
            "Episode 825# Avg Score: 83.96, Min: 19, Max: 165 \n",
            "Episode 850# Avg Score: 122.08, Min: 33, Max: 190 \n",
            "Episode 875# Avg Score: 236.2, Min: 17, Max: 501 \n",
            "Episode 900# Avg Score: 236.64, Min: 13, Max: 501 \n",
            "Episode 925# Avg Score: 213.44, Min: 22, Max: 501 \n",
            "Episode 950# Avg Score: 155.56, Min: 15, Max: 501 \n",
            "Episode 975# Avg Score: 116.52, Min: 10, Max: 501 \n",
            "Episode 1000# Avg Score: 181.92, Min: 9, Max: 501 \n",
            "Episode 1025# Avg Score: 145.56, Min: 100, Max: 186 \n",
            "Episode 1050# Avg Score: 151.56, Min: 32, Max: 343 \n",
            "Episode 1075# Avg Score: 167.64, Min: 33, Max: 298 \n",
            "Episode 1100# Avg Score: 128.0, Min: 45, Max: 185 \n",
            "Episode 1125# Avg Score: 122.32, Min: 20, Max: 172 \n",
            "Episode 1150# Avg Score: 101.44, Min: 12, Max: 243 \n",
            "Episode 1175# Avg Score: 116.84, Min: 9, Max: 432 \n",
            "Episode 1200# Avg Score: 147.28, Min: 20, Max: 501 \n",
            "Episode 1225# Avg Score: 69.24, Min: 9, Max: 249 \n",
            "Episode 1250# Avg Score: 128.8, Min: 12, Max: 501 \n",
            "Episode 1275# Avg Score: 126.64, Min: 16, Max: 221 \n",
            "Episode 1300# Avg Score: 117.16, Min: 50, Max: 173 \n",
            "Episode 1325# Avg Score: 129.88, Min: 47, Max: 370 \n",
            "Episode 1350# Avg Score: 115.4, Min: 9, Max: 233 \n",
            "Episode 1375# Avg Score: 38.04, Min: 11, Max: 139 \n",
            "Episode 1400# Avg Score: 72.4, Min: 11, Max: 200 \n",
            "Episode 1425# Avg Score: 88.0, Min: 23, Max: 211 \n",
            "Episode 1450# Avg Score: 141.44, Min: 54, Max: 335 \n",
            "Episode 1475# Avg Score: 135.4, Min: 94, Max: 219 \n",
            "Episode 1500# Avg Score: 125.44, Min: 103, Max: 167 \n",
            "Episode 1525# Avg Score: 150.2, Min: 67, Max: 244 \n",
            "Episode 1550# Avg Score: 133.8, Min: 34, Max: 259 \n",
            "Episode 1575# Avg Score: 90.16, Min: 24, Max: 240 \n",
            "Episode 1600# Avg Score: 86.32, Min: 12, Max: 244 \n",
            "Episode 1625# Avg Score: 121.44, Min: 22, Max: 280 \n",
            "Episode 1650# Avg Score: 170.92, Min: 136, Max: 408 \n",
            "Episode 1675# Avg Score: 215.0, Min: 137, Max: 501 \n",
            "Episode 1700# Avg Score: 205.28, Min: 120, Max: 501 \n",
            "Episode 1725# Avg Score: 153.56, Min: 93, Max: 233 \n",
            "Episode 1750# Avg Score: 106.52, Min: 17, Max: 166 \n",
            "Episode 1775# Avg Score: 111.88, Min: 93, Max: 152 \n",
            "Episode 1800# Avg Score: 86.28, Min: 12, Max: 188 \n",
            "Episode 1825# Avg Score: 64.52, Min: 19, Max: 114 \n",
            "Episode 1850# Avg Score: 101.08, Min: 32, Max: 206 \n",
            "Episode 1875# Avg Score: 76.96, Min: 10, Max: 146 \n",
            "Episode 1900# Avg Score: 116.64, Min: 23, Max: 184 \n",
            "Episode 1925# Avg Score: 126.32, Min: 13, Max: 159 \n",
            "Episode 1950# Avg Score: 113.84, Min: 44, Max: 139 \n",
            "Episode 1975# Avg Score: 90.92, Min: 21, Max: 129 \n",
            "Episode 2000# Avg Score: 111.76, Min: 21, Max: 150 \n",
            "Episode 2025# Avg Score: 113.48, Min: 94, Max: 255 \n",
            "Episode 2050# Avg Score: 113.24, Min: 95, Max: 155 \n",
            "Episode 2075# Avg Score: 115.16, Min: 94, Max: 178 \n",
            "Episode 2100# Avg Score: 118.84, Min: 101, Max: 160 \n",
            "Episode 2125# Avg Score: 106.0, Min: 71, Max: 131 \n",
            "Episode 2150# Avg Score: 106.04, Min: 89, Max: 151 \n",
            "Episode 2175# Avg Score: 129.08, Min: 18, Max: 403 \n",
            "Episode 2200# Avg Score: 113.96, Min: 9, Max: 205 \n",
            "Episode 2225# Avg Score: 100.8, Min: 16, Max: 139 \n",
            "Episode 2250# Avg Score: 97.68, Min: 15, Max: 118 \n",
            "Episode 2275# Avg Score: 64.08, Min: 12, Max: 162 \n",
            "Episode 2300# Avg Score: 106.8, Min: 13, Max: 176 \n",
            "Episode 2325# Avg Score: 45.52, Min: 9, Max: 148 \n",
            "Episode 2350# Avg Score: 153.0, Min: 119, Max: 327 \n",
            "Episode 2375# Avg Score: 134.48, Min: 111, Max: 159 \n",
            "Episode 2400# Avg Score: 120.48, Min: 68, Max: 199 \n",
            "Episode 2425# Avg Score: 138.56, Min: 42, Max: 218 \n",
            "Episode 2450# Avg Score: 134.96, Min: 98, Max: 205 \n",
            "Episode 2475# Avg Score: 110.84, Min: 21, Max: 277 \n",
            "Episode 2500# Avg Score: 121.72, Min: 12, Max: 167 \n",
            "Episode 2525# Avg Score: 131.8, Min: 33, Max: 152 \n",
            "Episode 2550# Avg Score: 124.36, Min: 19, Max: 169 \n",
            "Episode 2575# Avg Score: 84.92, Min: 22, Max: 175 \n",
            "Episode 2600# Avg Score: 124.28, Min: 24, Max: 148 \n",
            "Episode 2625# Avg Score: 137.6, Min: 103, Max: 205 \n",
            "Episode 2650# Avg Score: 131.48, Min: 99, Max: 182 \n",
            "Episode 2675# Avg Score: 144.04, Min: 106, Max: 196 \n",
            "Episode 2700# Avg Score: 140.88, Min: 35, Max: 270 \n",
            "Episode 2725# Avg Score: 191.48, Min: 64, Max: 311 \n",
            "Episode 2750# Avg Score: 138.04, Min: 12, Max: 224 \n",
            "Episode 2775# Avg Score: 119.16, Min: 10, Max: 159 \n",
            "Episode 2800# Avg Score: 109.56, Min: 14, Max: 163 \n",
            "Episode 2825# Avg Score: 129.16, Min: 92, Max: 197 \n",
            "Episode 2850# Avg Score: 149.16, Min: 121, Max: 199 \n",
            "Episode 2875# Avg Score: 147.52, Min: 13, Max: 209 \n",
            "Episode 2900# Avg Score: 175.64, Min: 119, Max: 316 \n",
            "Episode 2925# Avg Score: 167.64, Min: 33, Max: 241 \n",
            "Episode 2950# Avg Score: 170.8, Min: 58, Max: 237 \n",
            "Episode 2975# Avg Score: 172.24, Min: 96, Max: 222 \n",
            "Episode 3000# Avg Score: 175.92, Min: 79, Max: 337 \n",
            "Episode 3025# Avg Score: 134.48, Min: 14, Max: 236 \n",
            "Episode 3050# Avg Score: 174.72, Min: 50, Max: 289 \n",
            "Episode 3075# Avg Score: 186.08, Min: 28, Max: 501 \n",
            "Episode 3100# Avg Score: 171.0, Min: 12, Max: 210 \n",
            "Episode 3125# Avg Score: 212.32, Min: 19, Max: 501 \n",
            "Episode 3150# Avg Score: 90.92, Min: 11, Max: 501 \n",
            "Episode 3175# Avg Score: 83.84, Min: 9, Max: 444 \n",
            "Episode 3200# Avg Score: 145.44, Min: 14, Max: 468 \n",
            "Episode 3225# Avg Score: 143.92, Min: 44, Max: 501 \n",
            "Episode 3250# Avg Score: 38.72, Min: 10, Max: 126 \n",
            "Episode 3275# Avg Score: 57.4, Min: 13, Max: 279 \n",
            "Episode 3300# Avg Score: 59.64, Min: 15, Max: 199 \n",
            "Episode 3325# Avg Score: 108.44, Min: 12, Max: 330 \n",
            "Episode 3350# Avg Score: 128.2, Min: 10, Max: 317 \n",
            "Episode 3375# Avg Score: 222.32, Min: 24, Max: 501 \n",
            "Episode 3400# Avg Score: 195.84, Min: 14, Max: 501 \n",
            "Episode 3425# Avg Score: 237.92, Min: 194, Max: 287 \n",
            "Episode 3450# Avg Score: 216.48, Min: 100, Max: 358 \n",
            "Episode 3475# Avg Score: 441.44, Min: 42, Max: 501 \n",
            "Episode 3500# Avg Score: 349.84, Min: 250, Max: 501 \n",
            "Episode 3525# Avg Score: 306.56, Min: 227, Max: 405 \n",
            "Episode 3550# Avg Score: 383.16, Min: 77, Max: 501 \n",
            "Episode 3575# Avg Score: 501.0, Min: 501, Max: 501 \n",
            "Episode 3600# Avg Score: 439.52, Min: 42, Max: 501 \n",
            "Episode 3625# Avg Score: 476.24, Min: 39, Max: 501 \n",
            "Episode 3650# Avg Score: 498.92, Min: 449, Max: 501 \n",
            "Episode 3675# Avg Score: 393.24, Min: 18, Max: 501 \n",
            "Episode 3700# Avg Score: 398.72, Min: 277, Max: 501 \n",
            "Episode 3725# Avg Score: 475.88, Min: 355, Max: 501 \n",
            "Episode 3750# Avg Score: 483.28, Min: 197, Max: 501 \n",
            "Episode 3775# Avg Score: 494.48, Min: 400, Max: 501 \n",
            "Episode 3800# Avg Score: 286.24, Min: 9, Max: 501 \n",
            "Episode 3825# Avg Score: 278.92, Min: 167, Max: 478 \n",
            "Episode 3850# Avg Score: 355.36, Min: 14, Max: 501 \n",
            "Episode 3875# Avg Score: 259.36, Min: 11, Max: 438 \n",
            "Episode 3900# Avg Score: 379.32, Min: 215, Max: 501 \n",
            "Episode 3925# Avg Score: 392.92, Min: 332, Max: 462 \n",
            "Episode 3950# Avg Score: 450.56, Min: 149, Max: 501 \n",
            "Episode 3975# Avg Score: 218.68, Min: 9, Max: 501 \n",
            "Episode 4000# Avg Score: 330.24, Min: 50, Max: 501 \n",
            "Episode 4025# Avg Score: 261.0, Min: 11, Max: 501 \n",
            "Episode 4050# Avg Score: 237.72, Min: 36, Max: 501 \n",
            "Episode 4075# Avg Score: 351.84, Min: 91, Max: 501 \n",
            "Episode 4100# Avg Score: 446.04, Min: 10, Max: 501 \n",
            "Episode 4125# Avg Score: 422.56, Min: 9, Max: 501 \n",
            "Episode 4150# Avg Score: 215.68, Min: 14, Max: 501 \n",
            "Episode 4175# Avg Score: 200.64, Min: 14, Max: 501 \n",
            "Episode 4200# Avg Score: 267.12, Min: 10, Max: 501 \n",
            "Episode 4225# Avg Score: 153.8, Min: 12, Max: 501 \n",
            "Episode 4250# Avg Score: 58.8, Min: 9, Max: 500 \n",
            "Episode 4275# Avg Score: 177.28, Min: 34, Max: 348 \n",
            "Episode 4300# Avg Score: 164.28, Min: 81, Max: 501 \n",
            "Episode 4325# Avg Score: 211.92, Min: 136, Max: 501 \n",
            "Episode 4350# Avg Score: 229.2, Min: 19, Max: 501 \n",
            "Episode 4375# Avg Score: 319.52, Min: 56, Max: 501 \n",
            "Episode 4400# Avg Score: 462.04, Min: 130, Max: 501 \n",
            "Episode 4425# Avg Score: 350.28, Min: 20, Max: 501 \n",
            "Episode 4450# Avg Score: 181.04, Min: 52, Max: 236 \n",
            "Episode 4475# Avg Score: 136.28, Min: 18, Max: 387 \n",
            "Episode 4500# Avg Score: 433.68, Min: 62, Max: 501 \n",
            "Episode 4525# Avg Score: 372.6, Min: 13, Max: 501 \n",
            "Episode 4550# Avg Score: 422.52, Min: 69, Max: 501 \n",
            "Episode 4575# Avg Score: 389.6, Min: 9, Max: 501 \n",
            "Episode 4600# Avg Score: 323.32, Min: 12, Max: 501 \n",
            "Episode 4625# Avg Score: 148.72, Min: 10, Max: 263 \n",
            "Episode 4650# Avg Score: 68.28, Min: 10, Max: 410 \n",
            "Episode 4675# Avg Score: 156.36, Min: 10, Max: 273 \n",
            "Episode 4700# Avg Score: 171.4, Min: 110, Max: 486 \n",
            "Episode 4725# Avg Score: 162.52, Min: 101, Max: 249 \n",
            "Episode 4750# Avg Score: 163.68, Min: 116, Max: 501 \n",
            "Episode 4775# Avg Score: 196.8, Min: 145, Max: 501 \n",
            "Episode 4800# Avg Score: 239.76, Min: 163, Max: 501 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-93fdf27b2625>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcartpole\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCartPole\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcartpole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-2c22dc6f5a09>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m                          )\n\u001b[1;32m     46\u001b[0m                     \u001b[0msample_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-ea2bd8570faa>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, sample_batch_size)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;31m# entrenar: este estado (st ate) debe tener este valor (target_f)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# fit resta la estimacion actual, asi solo pasamos el target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# actualiza epsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}